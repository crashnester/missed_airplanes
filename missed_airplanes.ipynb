{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "missed_airplanes.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bAMAuKae_fDb"
      },
      "source": [
        "# Download/Install\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ePReGhO98cYu"
      },
      "source": [
        "# download main dataset and csv files\n",
        "!wget https://cups.mail.ru/api_v2/task/1089/89\n",
        "!wget https://cups.mail.ru/api_v2/task/1089/90\n",
        "!wget https://cups.mail.ru/api_v2/task/1089/83\n",
        "!wget https://cups.mail.ru/api_v2/task/1089/84\n",
        "!wget https://cups.mail.ru/api_v2/task/1089/85\n",
        "!mv 83 sample_submission.csv\n",
        "!mv 84 test.csv\n",
        "!mv 85 train.csv\n",
        "!unzip 89\n",
        "!unzip 90\n",
        "!rm 89\n",
        "!rm 90\n",
        "!rm -r __MACOSX"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y9BvqV1Z8qoq"
      },
      "source": [
        "# download additional dataset\n",
        "!wget https://zenodo.org/record/3464319/files/airplane-dataset-asoc.zip?download=1\n",
        "!unzip airplane-dataset-asoc.zip?download=1\n",
        "!rm -r airplane-dataset-asoc.zip?download=1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wJssCt3T-kzB",
        "outputId": "93bf9140-dc27-42e5-cbef-63cbfbaafa74"
      },
      "source": [
        "cd 'airplane-dataset-trans'"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/airplane-dataset-trans\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jyCne8UZ-oQx"
      },
      "source": [
        "!rm TYPE-NAMES.txt\n",
        "!rm names.txt\n",
        "!rm '#num.txt'\n",
        "!rm '#num.xlsx' "
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S_8DENzA-oW0",
        "outputId": "bdb8e13e-9d6c-4e9c-c8e1-df8cde4b57ee"
      },
      "source": [
        "cd .."
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "Ly8gQ29weXJpZ2h0IDIwMTcgR29vZ2xlIExMQwovLwovLyBMaWNlbnNlZCB1bmRlciB0aGUgQXBhY2hlIExpY2Vuc2UsIFZlcnNpb24gMi4wICh0aGUgIkxpY2Vuc2UiKTsKLy8geW91IG1heSBub3QgdXNlIHRoaXMgZmlsZSBleGNlcHQgaW4gY29tcGxpYW5jZSB3aXRoIHRoZSBMaWNlbnNlLgovLyBZb3UgbWF5IG9idGFpbiBhIGNvcHkgb2YgdGhlIExpY2Vuc2UgYXQKLy8KLy8gICAgICBodHRwOi8vd3d3LmFwYWNoZS5vcmcvbGljZW5zZXMvTElDRU5TRS0yLjAKLy8KLy8gVW5sZXNzIHJlcXVpcmVkIGJ5IGFwcGxpY2FibGUgbGF3IG9yIGFncmVlZCB0byBpbiB3cml0aW5nLCBzb2Z0d2FyZQovLyBkaXN0cmlidXRlZCB1bmRlciB0aGUgTGljZW5zZSBpcyBkaXN0cmlidXRlZCBvbiBhbiAiQVMgSVMiIEJBU0lTLAovLyBXSVRIT1VUIFdBUlJBTlRJRVMgT1IgQ09ORElUSU9OUyBPRiBBTlkgS0lORCwgZWl0aGVyIGV4cHJlc3Mgb3IgaW1wbGllZC4KLy8gU2VlIHRoZSBMaWNlbnNlIGZvciB0aGUgc3BlY2lmaWMgbGFuZ3VhZ2UgZ292ZXJuaW5nIHBlcm1pc3Npb25zIGFuZAovLyBsaW1pdGF0aW9ucyB1bmRlciB0aGUgTGljZW5zZS4KCi8qKgogKiBAZmlsZW92ZXJ2aWV3IEhlbHBlcnMgZm9yIGdvb2dsZS5jb2xhYiBQeXRob24gbW9kdWxlLgogKi8KKGZ1bmN0aW9uKHNjb3BlKSB7CmZ1bmN0aW9uIHNwYW4odGV4dCwgc3R5bGVBdHRyaWJ1dGVzID0ge30pIHsKICBjb25zdCBlbGVtZW50ID0gZG9jdW1lbnQuY3JlYXRlRWxlbWVudCgnc3BhbicpOwogIGVsZW1lbnQudGV4dENvbnRlbnQgPSB0ZXh0OwogIGZvciAoY29uc3Qga2V5IG9mIE9iamVjdC5rZXlzKHN0eWxlQXR0cmlidXRlcykpIHsKICAgIGVsZW1lbnQuc3R5bGVba2V5XSA9IHN0eWxlQXR0cmlidXRlc1trZXldOwogIH0KICByZXR1cm4gZWxlbWVudDsKfQoKLy8gTWF4IG51bWJlciBvZiBieXRlcyB3aGljaCB3aWxsIGJlIHVwbG9hZGVkIGF0IGEgdGltZS4KY29uc3QgTUFYX1BBWUxPQURfU0laRSA9IDEwMCAqIDEwMjQ7CgpmdW5jdGlvbiBfdXBsb2FkRmlsZXMoaW5wdXRJZCwgb3V0cHV0SWQpIHsKICBjb25zdCBzdGVwcyA9IHVwbG9hZEZpbGVzU3RlcChpbnB1dElkLCBvdXRwdXRJZCk7CiAgY29uc3Qgb3V0cHV0RWxlbWVudCA9IGRvY3VtZW50LmdldEVsZW1lbnRCeUlkKG91dHB1dElkKTsKICAvLyBDYWNoZSBzdGVwcyBvbiB0aGUgb3V0cHV0RWxlbWVudCB0byBtYWtlIGl0IGF2YWlsYWJsZSBmb3IgdGhlIG5leHQgY2FsbAogIC8vIHRvIHVwbG9hZEZpbGVzQ29udGludWUgZnJvbSBQeXRob24uCiAgb3V0cHV0RWxlbWVudC5zdGVwcyA9IHN0ZXBzOwoKICByZXR1cm4gX3VwbG9hZEZpbGVzQ29udGludWUob3V0cHV0SWQpOwp9CgovLyBUaGlzIGlzIHJvdWdobHkgYW4gYXN5bmMgZ2VuZXJhdG9yIChub3Qgc3VwcG9ydGVkIGluIHRoZSBicm93c2VyIHlldCksCi8vIHdoZXJlIHRoZXJlIGFyZSBtdWx0aXBsZSBhc3luY2hyb25vdXMgc3RlcHMgYW5kIHRoZSBQeXRob24gc2lkZSBpcyBnb2luZwovLyB0byBwb2xsIGZvciBjb21wbGV0aW9uIG9mIGVhY2ggc3RlcC4KLy8gVGhpcyB1c2VzIGEgUHJvbWlzZSB0byBibG9jayB0aGUgcHl0aG9uIHNpZGUgb24gY29tcGxldGlvbiBvZiBlYWNoIHN0ZXAsCi8vIHRoZW4gcGFzc2VzIHRoZSByZXN1bHQgb2YgdGhlIHByZXZpb3VzIHN0ZXAgYXMgdGhlIGlucHV0IHRvIHRoZSBuZXh0IHN0ZXAuCmZ1bmN0aW9uIF91cGxvYWRGaWxlc0NvbnRpbnVlKG91dHB1dElkKSB7CiAgY29uc3Qgb3V0cHV0RWxlbWVudCA9IGRvY3VtZW50LmdldEVsZW1lbnRCeUlkKG91dHB1dElkKTsKICBjb25zdCBzdGVwcyA9IG91dHB1dEVsZW1lbnQuc3RlcHM7CgogIGNvbnN0IG5leHQgPSBzdGVwcy5uZXh0KG91dHB1dEVsZW1lbnQubGFzdFByb21pc2VWYWx1ZSk7CiAgcmV0dXJuIFByb21pc2UucmVzb2x2ZShuZXh0LnZhbHVlLnByb21pc2UpLnRoZW4oKHZhbHVlKSA9PiB7CiAgICAvLyBDYWNoZSB0aGUgbGFzdCBwcm9taXNlIHZhbHVlIHRvIG1ha2UgaXQgYXZhaWxhYmxlIHRvIHRoZSBuZXh0CiAgICAvLyBzdGVwIG9mIHRoZSBnZW5lcmF0b3IuCiAgICBvdXRwdXRFbGVtZW50Lmxhc3RQcm9taXNlVmFsdWUgPSB2YWx1ZTsKICAgIHJldHVybiBuZXh0LnZhbHVlLnJlc3BvbnNlOwogIH0pOwp9CgovKioKICogR2VuZXJhdG9yIGZ1bmN0aW9uIHdoaWNoIGlzIGNhbGxlZCBiZXR3ZWVuIGVhY2ggYXN5bmMgc3RlcCBvZiB0aGUgdXBsb2FkCiAqIHByb2Nlc3MuCiAqIEBwYXJhbSB7c3RyaW5nfSBpbnB1dElkIEVsZW1lbnQgSUQgb2YgdGhlIGlucHV0IGZpbGUgcGlja2VyIGVsZW1lbnQuCiAqIEBwYXJhbSB7c3RyaW5nfSBvdXRwdXRJZCBFbGVtZW50IElEIG9mIHRoZSBvdXRwdXQgZGlzcGxheS4KICogQHJldHVybiB7IUl0ZXJhYmxlPCFPYmplY3Q+fSBJdGVyYWJsZSBvZiBuZXh0IHN0ZXBzLgogKi8KZnVuY3Rpb24qIHVwbG9hZEZpbGVzU3RlcChpbnB1dElkLCBvdXRwdXRJZCkgewogIGNvbnN0IGlucHV0RWxlbWVudCA9IGRvY3VtZW50LmdldEVsZW1lbnRCeUlkKGlucHV0SWQpOwogIGlucHV0RWxlbWVudC5kaXNhYmxlZCA9IGZhbHNlOwoKICBjb25zdCBvdXRwdXRFbGVtZW50ID0gZG9jdW1lbnQuZ2V0RWxlbWVudEJ5SWQob3V0cHV0SWQpOwogIG91dHB1dEVsZW1lbnQuaW5uZXJIVE1MID0gJyc7CgogIGNvbnN0IHBpY2tlZFByb21pc2UgPSBuZXcgUHJvbWlzZSgocmVzb2x2ZSkgPT4gewogICAgaW5wdXRFbGVtZW50LmFkZEV2ZW50TGlzdGVuZXIoJ2NoYW5nZScsIChlKSA9PiB7CiAgICAgIHJlc29sdmUoZS50YXJnZXQuZmlsZXMpOwogICAgfSk7CiAgfSk7CgogIGNvbnN0IGNhbmNlbCA9IGRvY3VtZW50LmNyZWF0ZUVsZW1lbnQoJ2J1dHRvbicpOwogIGlucHV0RWxlbWVudC5wYXJlbnRFbGVtZW50LmFwcGVuZENoaWxkKGNhbmNlbCk7CiAgY2FuY2VsLnRleHRDb250ZW50ID0gJ0NhbmNlbCB1cGxvYWQnOwogIGNvbnN0IGNhbmNlbFByb21pc2UgPSBuZXcgUHJvbWlzZSgocmVzb2x2ZSkgPT4gewogICAgY2FuY2VsLm9uY2xpY2sgPSAoKSA9PiB7CiAgICAgIHJlc29sdmUobnVsbCk7CiAgICB9OwogIH0pOwoKICAvLyBXYWl0IGZvciB0aGUgdXNlciB0byBwaWNrIHRoZSBmaWxlcy4KICBjb25zdCBmaWxlcyA9IHlpZWxkIHsKICAgIHByb21pc2U6IFByb21pc2UucmFjZShbcGlja2VkUHJvbWlzZSwgY2FuY2VsUHJvbWlzZV0pLAogICAgcmVzcG9uc2U6IHsKICAgICAgYWN0aW9uOiAnc3RhcnRpbmcnLAogICAgfQogIH07CgogIGNhbmNlbC5yZW1vdmUoKTsKCiAgLy8gRGlzYWJsZSB0aGUgaW5wdXQgZWxlbWVudCBzaW5jZSBmdXJ0aGVyIHBpY2tzIGFyZSBub3QgYWxsb3dlZC4KICBpbnB1dEVsZW1lbnQuZGlzYWJsZWQgPSB0cnVlOwoKICBpZiAoIWZpbGVzKSB7CiAgICByZXR1cm4gewogICAgICByZXNwb25zZTogewogICAgICAgIGFjdGlvbjogJ2NvbXBsZXRlJywKICAgICAgfQogICAgfTsKICB9CgogIGZvciAoY29uc3QgZmlsZSBvZiBmaWxlcykgewogICAgY29uc3QgbGkgPSBkb2N1bWVudC5jcmVhdGVFbGVtZW50KCdsaScpOwogICAgbGkuYXBwZW5kKHNwYW4oZmlsZS5uYW1lLCB7Zm9udFdlaWdodDogJ2JvbGQnfSkpOwogICAgbGkuYXBwZW5kKHNwYW4oCiAgICAgICAgYCgke2ZpbGUudHlwZSB8fCAnbi9hJ30pIC0gJHtmaWxlLnNpemV9IGJ5dGVzLCBgICsKICAgICAgICBgbGFzdCBtb2RpZmllZDogJHsKICAgICAgICAgICAgZmlsZS5sYXN0TW9kaWZpZWREYXRlID8gZmlsZS5sYXN0TW9kaWZpZWREYXRlLnRvTG9jYWxlRGF0ZVN0cmluZygpIDoKICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgJ24vYSd9IC0gYCkpOwogICAgY29uc3QgcGVyY2VudCA9IHNwYW4oJzAlIGRvbmUnKTsKICAgIGxpLmFwcGVuZENoaWxkKHBlcmNlbnQpOwoKICAgIG91dHB1dEVsZW1lbnQuYXBwZW5kQ2hpbGQobGkpOwoKICAgIGNvbnN0IGZpbGVEYXRhUHJvbWlzZSA9IG5ldyBQcm9taXNlKChyZXNvbHZlKSA9PiB7CiAgICAgIGNvbnN0IHJlYWRlciA9IG5ldyBGaWxlUmVhZGVyKCk7CiAgICAgIHJlYWRlci5vbmxvYWQgPSAoZSkgPT4gewogICAgICAgIHJlc29sdmUoZS50YXJnZXQucmVzdWx0KTsKICAgICAgfTsKICAgICAgcmVhZGVyLnJlYWRBc0FycmF5QnVmZmVyKGZpbGUpOwogICAgfSk7CiAgICAvLyBXYWl0IGZvciB0aGUgZGF0YSB0byBiZSByZWFkeS4KICAgIGxldCBmaWxlRGF0YSA9IHlpZWxkIHsKICAgICAgcHJvbWlzZTogZmlsZURhdGFQcm9taXNlLAogICAgICByZXNwb25zZTogewogICAgICAgIGFjdGlvbjogJ2NvbnRpbnVlJywKICAgICAgfQogICAgfTsKCiAgICAvLyBVc2UgYSBjaHVua2VkIHNlbmRpbmcgdG8gYXZvaWQgbWVzc2FnZSBzaXplIGxpbWl0cy4gU2VlIGIvNjIxMTU2NjAuCiAgICBsZXQgcG9zaXRpb24gPSAwOwogICAgZG8gewogICAgICBjb25zdCBsZW5ndGggPSBNYXRoLm1pbihmaWxlRGF0YS5ieXRlTGVuZ3RoIC0gcG9zaXRpb24sIE1BWF9QQVlMT0FEX1NJWkUpOwogICAgICBjb25zdCBjaHVuayA9IG5ldyBVaW50OEFycmF5KGZpbGVEYXRhLCBwb3NpdGlvbiwgbGVuZ3RoKTsKICAgICAgcG9zaXRpb24gKz0gbGVuZ3RoOwoKICAgICAgY29uc3QgYmFzZTY0ID0gYnRvYShTdHJpbmcuZnJvbUNoYXJDb2RlLmFwcGx5KG51bGwsIGNodW5rKSk7CiAgICAgIHlpZWxkIHsKICAgICAgICByZXNwb25zZTogewogICAgICAgICAgYWN0aW9uOiAnYXBwZW5kJywKICAgICAgICAgIGZpbGU6IGZpbGUubmFtZSwKICAgICAgICAgIGRhdGE6IGJhc2U2NCwKICAgICAgICB9LAogICAgICB9OwoKICAgICAgbGV0IHBlcmNlbnREb25lID0gZmlsZURhdGEuYnl0ZUxlbmd0aCA9PT0gMCA/CiAgICAgICAgICAxMDAgOgogICAgICAgICAgTWF0aC5yb3VuZCgocG9zaXRpb24gLyBmaWxlRGF0YS5ieXRlTGVuZ3RoKSAqIDEwMCk7CiAgICAgIHBlcmNlbnQudGV4dENvbnRlbnQgPSBgJHtwZXJjZW50RG9uZX0lIGRvbmVgOwoKICAgIH0gd2hpbGUgKHBvc2l0aW9uIDwgZmlsZURhdGEuYnl0ZUxlbmd0aCk7CiAgfQoKICAvLyBBbGwgZG9uZS4KICB5aWVsZCB7CiAgICByZXNwb25zZTogewogICAgICBhY3Rpb246ICdjb21wbGV0ZScsCiAgICB9CiAgfTsKfQoKc2NvcGUuZ29vZ2xlID0gc2NvcGUuZ29vZ2xlIHx8IHt9OwpzY29wZS5nb29nbGUuY29sYWIgPSBzY29wZS5nb29nbGUuY29sYWIgfHwge307CnNjb3BlLmdvb2dsZS5jb2xhYi5fZmlsZXMgPSB7CiAgX3VwbG9hZEZpbGVzLAogIF91cGxvYWRGaWxlc0NvbnRpbnVlLAp9Owp9KShzZWxmKTsK",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "id": "EJV-TagD-oct",
        "outputId": "c86c31b8-0d0e-4f29-8e05-f6b53e9c4b94"
      },
      "source": [
        "# kaggle.json\n",
        "from google.colab import files\n",
        " \n",
        "files.upload()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-99815f83-62f0-4617-bf3e-a9bbcd3794e2\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-99815f83-62f0-4617-bf3e-a9bbcd3794e2\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "text": [
            "Saving kaggle.json to kaggle.json\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'kaggle.json': b'{\"username\":\"crashnester\",\"key\":\"e8f1f979c1799d93dd1898a7730ae479\"}'}"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "diIKDgFi-53U"
      },
      "source": [
        "!mkdir ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "!pip uninstall -y kaggle\n",
        "!pip install --upgrade pip\n",
        "!pip install kaggle==1.5.6"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cs556K3W-9RD"
      },
      "source": [
        "# dowload pre-trained efficientnet\n",
        "!kaggle datasets download -d hmendonca/efficientnet-pytorch\n",
        "!unzip efficientnet-pytorch.zip\n",
        "!rm -r efficientnet-pytorch.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4VOarxSc5khi"
      },
      "source": [
        "# Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7wh-V3iq5fzs"
      },
      "source": [
        "import os\n",
        "import sys\n",
        "sys.path = ['EfficientNet-PyTorch/EfficientNet-PyTorch-master',] + sys.path\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import glob\n",
        "import imageio\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import metrics\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "import torchvision.models as models\n",
        "import torch.nn as nn\n",
        "from efficientnet_pytorch import model as enet\n",
        "import random\n",
        "from sklearn.model_selection import KFold, StratifiedKFold\n",
        "import albumentations as A"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w2X8v5CTAa1O"
      },
      "source": [
        "def set_seed(seed = 0):\n",
        "    '''Sets the seed of the entire notebook so results are the same every time we run.\n",
        "    This is for REPRODUCIBILITY.'''\n",
        "    np.random.seed(seed)\n",
        "    random_state = np.random.RandomState(seed)\n",
        "    random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    return random_state\n",
        " \n",
        "seed = 42\n",
        "random_state = set_seed(seed)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8-tkzl5aAeEC",
        "outputId": "be29e2da-d548-47ec-e77c-78d3fc7e1795"
      },
      "source": [
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "    print(\"GPU is available\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    print(\"GPU not available, CPU used\")"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "GPU is available\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wSgZghdnA9Oh"
      },
      "source": [
        "# Augmentation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ARX-Sma1AhVA"
      },
      "source": [
        "img_sz = 224\n",
        "\n",
        "ttransform = A.Compose([\n",
        "    A.Resize(img_sz, img_sz, cv2.INTER_NEAREST),\n",
        "    A.VerticalFlip(p=0.5),\n",
        "    A.HorizontalFlip(p=0.5),\n",
        "    A.Rotate(limit=180, p=0.5),\n",
        "    A.RandomBrightness(p=0.5),\n",
        "])\n",
        "\n",
        "vtransform = A.Compose([\n",
        "    A.Resize(img_sz, img_sz, cv2.INTER_NEAREST),\n",
        "])"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m3h5VUYF5ggy"
      },
      "source": [
        "class ClassificationDataset:\n",
        "    def __init__(self, image_paths, targets, tr=None, train=True): \n",
        "        self.image_paths = image_paths\n",
        "        self.targets = targets\n",
        "        self.train = train\n",
        "        self.tr = tr\n",
        "         \n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "    \n",
        "    def __getitem__(self, item):      \n",
        "        image = imageio.imread(self.image_paths[item])\n",
        "        image = image[:,:,:3]\n",
        "        targets = self.targets[item]\n",
        "     \n",
        "        image = self.tr(image = image)[\"image\"].transpose((2, 1, 0)).astype(np.float32)\n",
        "        image = image / 255.0\n",
        "                \n",
        "        return {\n",
        "            \"image\": torch.tensor(image, dtype=torch.float),\n",
        "            \"targets\": torch.tensor(targets, dtype=torch.long),\n",
        "        }"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7qlyLkevCN0G",
        "outputId": "a50bab49-9837-4f23-e498-3cdb22d47fdd"
      },
      "source": [
        "# read additional dataset\n",
        "paths_list = []\n",
        "\n",
        "os.chdir('.')\n",
        "for root, dirs, files in os.walk('airplane-dataset-trans/', topdown = False):\n",
        "   for name in files:\n",
        "        paths_list.append(os.path.join(root, name))\n",
        "\n",
        "additional_paths = pd.DataFrame({'sign': 1, 'img_path': paths_list})\n",
        "for i in range(len(additional_paths)):\n",
        "    if 'backgroud-2' in additional_paths.img_path.iloc[i] or \\\n",
        "        'baground' in additional_paths.img_path.iloc[i] or \\\n",
        "        'baground-1' in additional_paths.img_path.iloc[i] :\n",
        "        additional_paths.sign.iloc[i] = 0"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/pandas/core/indexing.py:670: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  iloc._setitem_with_indexer(indexer, value)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4cJBcEzpCL5Q"
      },
      "source": [
        "df = pd.read_csv('train.csv')\n",
        "df['img_path'] = df['filename'].apply(lambda x: 'avia-train/' + x + '.png')\n",
        "df = df.drop(['filename'], axis=1)\n",
        "df = df.append(additional_paths)\n",
        "df = df.sample(frac=1).reset_index(drop=True)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZG8Lng2i7JdC"
      },
      "source": [
        "class enetv2(nn.Module):\n",
        "    def __init__(self, backbone, out_dim):\n",
        "        super(enetv2, self).__init__()\n",
        "        self.enet = enet.EfficientNet.from_name(backbone)\n",
        "        self.enet.load_state_dict(torch.load(pretrained_model[backbone]))\n",
        "\n",
        "        self.myfc = nn.Linear(self.enet._fc.in_features, out_dim)\n",
        "        self.enet._fc = nn.Identity()\n",
        "        self.conv1 = nn.Conv2d(3, 3, kernel_size=3, stride=1, padding=3, bias=False)\n",
        " \n",
        "    def extract(self, x):\n",
        "        return self.enet(x)\n",
        " \n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.extract(x)\n",
        "        x = self.myfc(x)\n",
        "        \n",
        "        return x"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Epmb3ZT47Jvb"
      },
      "source": [
        "def mixup_data(x, y, alpha=1.0, use_cuda=True):\n",
        "    if alpha > 0:\n",
        "        lam = np.random.beta(alpha, alpha)\n",
        "    else:\n",
        "        lam = 1\n",
        " \n",
        "    batch_size = x.size()[0]\n",
        "    if use_cuda:\n",
        "        index = torch.randperm(batch_size).cuda()\n",
        "    else:\n",
        "        index = torch.randperm(batch_size)\n",
        " \n",
        "    mixed_x = lam * x + (1 - lam) * x[index, :]\n",
        "    y_a, y_b = y, y[index]\n",
        "    return mixed_x, y_a, y_b, lam\n",
        " \n",
        " \n",
        "def mixup_criterion(criterion, pred, y_a, y_b, lam):\n",
        "    return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bs2M0KJq7QW5"
      },
      "source": [
        "def train(data_loader, model, optimizer, device):\n",
        "    model.train()\n",
        "    \n",
        "    for data in tqdm(data_loader, position=0, leave=True, desc='Training'):\n",
        "        inputs = data['image']\n",
        "        targets = data['targets']\n",
        "        \n",
        "        inputs, targets_a, targets_b, lam = mixup_data(inputs, targets.view(-1, 1), use_cuda=True)\n",
        "        inputs = inputs.to(device, dtype=torch.float)\n",
        "        targets_a = targets_a.to(device, dtype=torch.float)\n",
        "        targets_b = targets_b.to(device, dtype=torch.float)\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = mixup_criterion(criterion, outputs, targets_a, targets_b, lam)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "def evaluate(data_loader, model, device):\n",
        "    model.eval()\n",
        "    \n",
        "    final_targets = []\n",
        "    final_outputs = []\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for data in tqdm(data_loader, position=0, leave=True, desc='Evaluating'):\n",
        "            inputs = data['image']\n",
        "            targets = data['targets']\n",
        "            \n",
        "            inputs = inputs.to(device, dtype=torch.float)\n",
        "            targets = targets.to(device, dtype=torch.float)\n",
        "            \n",
        "            output = model(inputs)\n",
        "            output = torch.sigmoid(output)\n",
        "            targets = targets.detach().cpu().numpy().tolist()\n",
        "            output = output.detach().cpu().numpy().tolist()\n",
        "            \n",
        "            final_targets.extend(targets)\n",
        "            final_outputs.extend(output)\n",
        "            \n",
        "            del output, targets, inputs\n",
        "            \n",
        "    return final_outputs, final_targets"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "csBvwF4oChYo"
      },
      "source": [
        "paths = [\n",
        " 'efficientnet-b0-08094119.pth',\n",
        " 'efficientnet-b1-dbc7070a.pth',\n",
        " 'efficientnet-b2-27687264.pth',\n",
        " 'efficientnet-b3-c8376fa2.pth',\n",
        " 'efficientnet-b4-e116e8b3.pth',\n",
        " 'efficientnet-b5-586e6cc6.pth',\n",
        " 'efficientnet-b6-c76e70fd.pth',\n",
        " 'efficientnet-b7-dcc49843.pth',\n",
        "]"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AXRstCszC9IK"
      },
      "source": [
        "# Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p6SZ6dYZ7b5J"
      },
      "source": [
        "baseline_name = 'efficientnet-b4'\n",
        "pretrained_model = {\n",
        "    'efficientnet-b4': paths[4]\n",
        "}\n",
        " \n",
        "epochs = 25\n",
        "batch_size = 32\n",
        "X = df.img_path.values\n",
        "Y = df.sign.values\n",
        "skf = StratifiedKFold(n_splits=5)\n",
        "fold = 0\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "for train_index, test_index in skf.split(X, Y):\n",
        "    model = enetv2(baseline_name, out_dim=1)\n",
        "    model.to(device)\n",
        " \n",
        "    train_images, valid_images = X[train_index], X[test_index]\n",
        "    train_targets, valid_targets = Y[train_index], Y[test_index]\n",
        " \n",
        "    train_dataset = ClassificationDataset(image_paths=train_images, targets=train_targets, tr=ttransform)\n",
        "    valid_dataset = ClassificationDataset(image_paths=valid_images, targets=valid_targets, tr=vtransform)\n",
        "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
        "    valid_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
        " \n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=5e-4)\n",
        " \n",
        "    best_score = 0\n",
        "    for epoch in range(epochs):\n",
        "        print(f'\\nepoch {epoch + 1}')\n",
        "        train(train_loader, model, optimizer, device=device)\n",
        "        predictions, valid_targets = evaluate(valid_loader, model, device=device)\n",
        "        roc_auc = metrics.roc_auc_score(valid_targets, predictions)\n",
        " \n",
        "        if roc_auc > best_score:\n",
        "            torch.save(model.state_dict(), f'models/{baseline_name}-{str(fold)}.pt')\n",
        "            best_score = roc_auc\n",
        "        print(f'Valid ROC AUC={roc_auc} (best {best_score})')\n",
        " \n",
        "    print(f'best score: {best_score}\\n\\n')\n",
        "    fold += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kNoDHWhTDKpf"
      },
      "source": [
        "# Evaluate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "QTCZlzdSDE3d",
        "outputId": "e40d1e83-28da-45cc-8277-dc6144f87843"
      },
      "source": [
        "test_data = pd.read_csv('test.csv')\n",
        "test_data['img_path'] = test_data['filename'].apply(lambda x: 'avia-test/' + x + '.png')\n",
        "test_data = test_data.drop(['filename'], axis=1)\n",
        "test_data.head()"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>img_path</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>avia-test/68f33844-472b-4111-b600-f90d544833c7...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>avia-test/7d93a21d-1f16-49ce-8fcc-edf12c40f549...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>avia-test/4a820650-7acd-489a-ad14-9d7ad8c73b6b...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>avia-test/819b216b-2b6c-4539-a722-70648c0706c6...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>avia-test/45f7c47d-03cc-40cd-acc5-b8c1c57872fa...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                            img_path\n",
              "0  avia-test/68f33844-472b-4111-b600-f90d544833c7...\n",
              "1  avia-test/7d93a21d-1f16-49ce-8fcc-edf12c40f549...\n",
              "2  avia-test/4a820650-7acd-489a-ad14-9d7ad8c73b6b...\n",
              "3  avia-test/819b216b-2b6c-4539-a722-70648c0706c6...\n",
              "4  avia-test/45f7c47d-03cc-40cd-acc5-b8c1c57872fa..."
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WK5cHMPeDTce"
      },
      "source": [
        "device = \"cuda\"\n",
        "baseline_name = 'efficientnet-b4'\n",
        "\n",
        "models = []\n",
        "for i in range(fold):\n",
        "    model = enetv2(baseline_name, out_dim=1)\n",
        "    model.to(device)\n",
        "    model.load_state_dict(torch.load(f'models/{baseline_name}-{i}.pt'))\n",
        "\n",
        "    models.append(model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x9Bz29VxDYtL"
      },
      "source": [
        "sample_submission = pd.read_csv('sample_submission.csv')"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zMLf6jNcDanZ"
      },
      "source": [
        "preds = []\n",
        "for each in models:\n",
        "    test_dataset = ClassificationDataset(image_paths=test_data.img_path.values, targets=sample_submission.sign.values, tr=vtransform)\n",
        "    test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
        "    predictions, valid_targets = evaluate(test_loader, each, device=device)\n",
        "    preds.append(predictions)\n",
        "\n",
        "outputs = np.mean(preds, axis = 0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ahvd2WotDhOE"
      },
      "source": [
        "# Submission"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KDbVWO4zDgcA"
      },
      "source": [
        "sample_submission['sign'] = outputs\n",
        "sample_submission.to_csv('submission_b4_224.csv', index=False)\n",
        "sample_submission.head()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}